{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:19.357100Z",
     "iopub.status.busy": "2022-02-25T02:43:19.356721Z",
     "iopub.status.idle": "2022-02-25T02:43:20.030510Z",
     "shell.execute_reply": "2022-02-25T02:43:20.029600Z",
     "shell.execute_reply.started": "2022-02-25T02:43:19.357057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:20.033069Z",
     "iopub.status.busy": "2022-02-25T02:43:20.032562Z",
     "iopub.status.idle": "2022-02-25T02:43:20.040938Z",
     "shell.execute_reply": "2022-02-25T02:43:20.039939Z",
     "shell.execute_reply.started": "2022-02-25T02:43:20.033012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:20.044097Z",
     "iopub.status.busy": "2022-02-25T02:43:20.043354Z",
     "iopub.status.idle": "2022-02-25T02:43:28.557936Z",
     "shell.execute_reply": "2022-02-25T02:43:28.557122Z",
     "shell.execute_reply.started": "2022-02-25T02:43:20.044058Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:28.560432Z",
     "iopub.status.busy": "2022-02-25T02:43:28.560128Z",
     "iopub.status.idle": "2022-02-25T02:43:33.582170Z",
     "shell.execute_reply": "2022-02-25T02:43:33.581381Z",
     "shell.execute_reply.started": "2022-02-25T02:43:28.560394Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:33.584684Z",
     "iopub.status.busy": "2022-02-25T02:43:33.584493Z",
     "iopub.status.idle": "2022-02-25T02:43:37.735888Z",
     "shell.execute_reply": "2022-02-25T02:43:37.735121Z",
     "shell.execute_reply.started": "2022-02-25T02:43:33.584660Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip;\n",
    "# ! unzip ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip;\n",
    "# ! unzip ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip;\n",
    "# ! unzip ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:37.738918Z",
     "iopub.status.busy": "2022-02-25T02:43:37.738390Z",
     "iopub.status.idle": "2022-02-25T02:43:40.296979Z",
     "shell.execute_reply": "2022-02-25T02:43:40.296241Z",
     "shell.execute_reply.started": "2022-02-25T02:43:37.738875Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./dataset\"\n",
    "bert_model_name = 'bert-base-cased'\n",
    "# path = \"../input/jigsaw-toxic-comment-classification-challenge/\"\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "assert tokenizer.pad_token_id == 0, \"Padding value used in masks is set to zero, please change it everywhere\"\n",
    "train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "# training on a part of data for speed\n",
    "# train_df = train_df.sample(frac=0.33)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:40.298393Z",
     "iopub.status.busy": "2022-02-25T02:43:40.298143Z",
     "iopub.status.idle": "2022-02-25T02:43:40.315874Z",
     "shell.execute_reply": "2022-02-25T02:43:40.315079Z",
     "shell.execute_reply.started": "2022-02-25T02:43:40.298359Z"
    }
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer: BertTokenizer, dataframe: pd.DataFrame, lazy: bool = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = tokenizer.pad_token_id\n",
    "        self.lazy = lazy\n",
    "        if not self.lazy:\n",
    "            self.X = []\n",
    "            self.Y = []\n",
    "            for i, (row) in tqdm(dataframe.iterrows()):\n",
    "                x, y = self.row_to_tensor(self.tokenizer, row)\n",
    "                self.X.append(x)\n",
    "                self.Y.append(y)\n",
    "        else:\n",
    "            self.df = dataframe        \n",
    "    \n",
    "    @staticmethod\n",
    "    def row_to_tensor(tokenizer: BertTokenizer, row: pd.Series) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        tokens = tokenizer.encode(row[\"comment_text\"], add_special_tokens=True)\n",
    "        if len(tokens) > 120:\n",
    "            tokens = tokens[:119] + [tokens[-1]]\n",
    "        x = torch.LongTensor(tokens)\n",
    "        y = torch.FloatTensor(row[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.lazy:\n",
    "            return len(self.df)\n",
    "        else:\n",
    "            return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        if not self.lazy:\n",
    "            return self.X[index], self.Y[index]\n",
    "        else:\n",
    "            return self.row_to_tensor(self.tokenizer, self.df.iloc[index])\n",
    "            \n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]], device: torch.device) \\\n",
    "        -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    x, y = list(zip(*batch))\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = torch.stack(y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "train_dataset = ToxicDataset(tokenizer, train_df, lazy=True)\n",
    "dev_dataset = ToxicDataset(tokenizer, val_df, lazy=True)\n",
    "collate_fn = partial(collate_fn, device=device)\n",
    "BATCH_SIZE = 32\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = RandomSampler(dev_dataset)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:43:40.317640Z",
     "iopub.status.busy": "2022-02-25T02:43:40.317309Z",
     "iopub.status.idle": "2022-02-25T02:44:08.041623Z",
     "shell.execute_reply": "2022-02-25T02:44:08.040839Z",
     "shell.execute_reply.started": "2022-02-25T02:43:40.317602Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert: BertModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                \n",
    "            labels=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "        cls_output = outputs[1] # batch, hidden\n",
    "        cls_output = self.classifier(cls_output) # batch, 6\n",
    "        cls_output = torch.sigmoid(cls_output)\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output, labels)\n",
    "        return loss, cls_output\n",
    "\n",
    "# model = BertClassifier(BertModel.from_pretrained(bert_model_name), 6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:44:08.043364Z",
     "iopub.status.busy": "2022-02-25T02:44:08.043038Z",
     "iopub.status.idle": "2022-02-25T02:44:08.053583Z",
     "shell.execute_reply": "2022-02-25T02:44:08.052534Z",
     "shell.execute_reply.started": "2022-02-25T02:44:08.043326Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        mask = (x != 0).float()\n",
    "        loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Train loss {total_loss / len(iterator)}\")\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(iterator):\n",
    "            mask = (x != 0).float()\n",
    "            loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "            total_loss += loss\n",
    "            true += y.cpu().numpy().tolist()\n",
    "            pred += outputs.cpu().numpy().tolist()\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    for i, name in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "        print(f\"{name} roc_auc {roc_auc_score(true[:, i], pred[:, i])}\")\n",
    "    print(f\"Evaluate loss {total_loss / len(iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:44:08.055493Z",
     "iopub.status.busy": "2022-02-25T02:44:08.055161Z",
     "iopub.status.idle": "2022-02-25T02:44:08.071647Z",
     "shell.execute_reply": "2022-02-25T02:44:08.070864Z",
     "shell.execute_reply.started": "2022-02-25T02:44:08.055450Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13605/3397567290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mno_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LayerNorm.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer_grouped_parameters = [\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mno_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mno_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "EPOCH_NUM = 2\n",
    "# triangular learning rate, linearly grows untill half of first epoch, then linearly decays \n",
    "warmup_steps = 10 ** 3\n",
    "total_steps = len(train_iterator) * EPOCH_NUM - warmup_steps\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T02:44:08.075340Z",
     "iopub.status.busy": "2022-02-25T02:44:08.074444Z",
     "iopub.status.idle": "2022-02-25T03:58:00.259825Z",
     "shell.execute_reply": "2022-02-25T03:58:00.259036Z",
     "shell.execute_reply.started": "2022-02-25T02:44:08.075306Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(EPOCH_NUM):\n",
    "    print('=' * 50, f\"EPOCH {i}\", '=' * 50)\n",
    "    train(model, train_iterator, optimizer, scheduler)\n",
    "    evaluate(model, dev_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T03:58:00.261464Z",
     "iopub.status.busy": "2022-02-25T03:58:00.261055Z",
     "iopub.status.idle": "2022-02-25T04:12:57.352435Z",
     "shell.execute_reply": "2022-02-25T04:12:57.351560Z",
     "shell.execute_reply.started": "2022-02-25T03:58:00.261423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/4787 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = igetitem(value, i)\n",
      "  0%|                                                                                 | 1/4787 [00:00<09:14,  8.64it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1052 > 512). Running this sequence through the model will result in indexing errors\n",
      " 59%|█████████████████████████████████████████████▉                                | 2819/4787 [04:46<03:13, 10.16it/s]"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_df = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
    "columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for i in tqdm(range(len(test_df) // BATCH_SIZE + 1)):\n",
    "    batch_df = test_df.iloc[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "    assert (batch_df[\"id\"] == submission[\"id\"][i * BATCH_SIZE: (i + 1) * BATCH_SIZE]).all(), f\"Id mismatch\"\n",
    "    texts = []\n",
    "    for text in batch_df[\"comment_text\"].tolist():\n",
    "        text = tokenizer.encode(text, add_special_tokens=True)\n",
    "        if len(text) > 120:\n",
    "            text = text[:119] + [tokenizer.sep_token_id]\n",
    "        texts.append(torch.LongTensor(text))\n",
    "    x = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
    "    mask = (x != tokenizer.pad_token_id).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(x, attention_mask=mask)\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    submission.iloc[i * BATCH_SIZE: (i + 1) * BATCH_SIZE][columns] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T04:12:57.354507Z",
     "iopub.status.busy": "2022-02-25T04:12:57.353667Z",
     "iopub.status.idle": "2022-02-25T04:12:58.189000Z",
     "shell.execute_reply": "2022-02-25T04:12:58.188174Z",
     "shell.execute_reply.started": "2022-02-25T04:12:57.354460Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './toxicity_model/model.pt')\n",
    "torch.save(tokenizer, './toxicity_model/toknizer.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./toxicity_model/model.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import BertClassifier\n",
    "model = BertClassifier(BertModel.from_pretrained(bert_model_name), 6)\n",
    "state_dict = torch.load('./toxicity_model/model.pt', map_location=device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T04:12:58.194251Z",
     "iopub.status.busy": "2022-02-25T04:12:58.194001Z",
     "iopub.status.idle": "2022-02-25T04:12:58.197956Z",
     "shell.execute_reply": "2022-02-25T04:12:58.197234Z",
     "shell.execute_reply.started": "2022-02-25T04:12:58.194218Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13605/2164528218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fucking environment :)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m119\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "input_text = 'fucking environment :)'\n",
    "text = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "if len(text) > 120:\n",
    "    text = text[:119] + [tokenizer.sep_token_id]\n",
    "texts.append(torch.LongTensor(text))\n",
    "x = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
    "mask = (x != tokenizer.pad_token_id).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(input_text, max_length=150, pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')\n",
    "# preprocessing text for question_answering.\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T04:34:36.160791Z",
     "iopub.status.busy": "2022-02-25T04:34:36.160530Z",
     "iopub.status.idle": "2022-02-25T04:34:36.201615Z",
     "shell.execute_reply": "2022-02-25T04:34:36.200399Z",
     "shell.execute_reply.started": "2022-02-25T04:34:36.160761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9719, 0.1290, 0.9651, 0.0062, 0.2454, 0.0118]], device='cuda:0')\n",
      "[97.193245  12.904939  96.50695    0.6226634 24.535187   1.1799219] [97.2 12.9 96.5  0.6 24.5  1.2]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    _, output = model(x,attention_mask=mask)\n",
    "#     _, output = model(input_ids,attention_mask=attention_mask)\n",
    "print(output)\n",
    "output = output[0].cpu().numpy()*100\n",
    "print(output, np.round(output,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
